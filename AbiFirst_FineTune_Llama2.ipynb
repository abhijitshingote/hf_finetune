{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69973a05-bef0-4cdd-bcb2-b0ffa1a8fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade torch==2.2.0\n",
    "pip install accelerate peft bitsandbytes transformers trl tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fce8e04-158d-4068-9b7b-0db496c973c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd14237a-4369-4b0f-99b6-8cd2a26e1eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model from Hugging Face hub\n",
    "base_model = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "# Fine-tuned model\n",
    "new_model = \"llama-2-7b-finetuned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66b90556-8343-475c-a04b-96bd0e0602b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New instruction dataset\n",
    "# guanaco_dataset = \"mlabonne/guanaco-llama2-1k\"\n",
    "# dataset = load_dataset(guanaco_dataset, split=\"train\")\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files=\"openassistant_best_replies_train.jsonl\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95324ade-71a1-447c-b661-58866a552f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '### Human: ¿CUales son las etapas del desarrollo y en qué consisten según Piaget?### Assistant: Jean Piaget fue un psicólogo suizo que propuso una teoría sobre el desarrollo cognitivo humano que consta de cuatro etapas:\\n\\nEtapa sensoriomotora (0-2 años): Durante esta etapa, el niño aprende a través de sus sentidos y movimientos. Descubre que sus acciones pueden tener un impacto en el entorno y comienza a formarse una idea básica de objetividad y continuidad.\\n\\nEtapa preoperatoria (2-7 años): En esta etapa, el niño comienza a desarrollar un pensamiento simbólico y a comprender que las cosas pueden representar a otras cosas. También comienzan a desarrollar un pensamiento lógico y a comprender conceptos como la causa y el efecto.\\n\\nEtapa de operaciones concretas (7-12 años): Durante esta etapa, el niño desarrolla un pensamiento lógico y comprende las relaciones causales. Empiezan a comprender que las cosas pueden tener múltiples perspectivas y que los conceptos pueden ser más complejos de lo que parecen a simple vista.\\n\\nEtapa de operaciones formales (12 años en adelante): En esta etapa, el individuo desarrolla un pensamiento abstracto y puede comprender conceptos complejos y abstractos. Son capaces de razonar hipotéticamente y tienen la capacidad de reflexionar sobre su propio pensamiento.\\n\\nEstas etapas no son lineales y algunos individuos pueden avanzar en una etapa más rápidamente que en otras. La teoría de Piaget sobre el desarrollo cognitivo ha sido ampliamente utilizada y es una base importante para la investigación y el entendimiento del desarrollo humano.### Human: ¿Hay otras teorías sobre las etapas del desarrollo que reafirmen o contradigan a la teoría de Piaget?'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56ce35b9-9d45-4ed2-a653-40930d8eb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_dtype = getattr(torch, \"float16\")\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d20b8507-23d3-4973-9ae2-aabbb57ae3c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e286dd916534f22a5369cd4e24a4fef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/609 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6627da8ee976499ea0775bfe1a3e6f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b7bc06eb9b47faa037dc820722a74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd32d1606b9946aa80a50b7f0115f3ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96715e0cb0b44c0d8004bc9fe5484304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8ff0e6a0a14053a5599f9208235a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd26d07721834235a205700e24244e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/188 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=quant_config,\n",
    "    device_map={\"\": 0},\n",
    "    token=hf_token\n",
    ")\n",
    "model.config.use_cache = False\n",
    "# model.config.pretraining_tp = 1??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0156923-8e0e-4836-98bc-66b966fe330f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98d1884ab77241108e5d5488932d35c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd2115cebbf45dc9c1ab3be53123ead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0def7e5532e4c2c8f7c294b925f3679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5298fe2cccca4c119916b86a2a77bdb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True,token=hf_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9857e840-3d88-411d-a7af-174a64ed4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_cuda_memory():\n",
    "    print(\"\\n--------------------------------------------------\\n\")\n",
    "    print(\"torch.cuda.memory_allocated: %fGB\"%(torch.cuda.memory_allocated(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.memory_reserved: %fGB\"%(torch.cuda.memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"torch.cuda.max_memory_reserved: %fGB\"%(torch.cuda.max_memory_reserved(0)/1024/1024/1024))\n",
    "    print(\"\\n--------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "221f343e-66a7-4b71-8195-6adb000267ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "torch.cuda.memory_allocated: 3.990852GB\n",
      "torch.cuda.memory_reserved: 4.087891GB\n",
      "torch.cuda.max_memory_reserved: 4.087891GB\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_cuda_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1133b00-2ebb-45e7-b290-c199ef066b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7832"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max([len(tokenizer.encode(dataset[i]['text'])) for i in range(1000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff70b4f7-66a1-4157-9cbf-662bbb745109",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_params = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0aa78824-101c-4e5f-bf18-2e12646eba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    save_steps=25,\n",
    "    logging_steps=25,\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=False,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    report_to=\"tensorboard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "391342c2-7d4f-49d8-a42f-e839ece7ee36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96c92f06f5d499db755dceda54253be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1000,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd3afae-5d3e-46ac-a40f-b946f1a53c3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='108' max='2462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 108/2462 05:37 < 2:04:54, 0.31 it/s, Epoch 0.04/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.239200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.443300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.092700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.397500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6638a03c-04a9f1037eae114d312e52d6;96eff391-ff20-40d5-a240-8b329e02ded3)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6638a063-33f04b83749dbde66b3c41f0;abe261a4-0214-4028-b8d4-1778d8d57586)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6638a0cf-3c6665d35c317cca06856546;81005a53-d5ff-48d2-97ca-03599496a8ab)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:581: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-6638a0f8-08ed534e73c0351871ca38fa;a4528120-d300-4250-9747-04842f294c96)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Access to model meta-llama/Llama-2-7b-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:154: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9e7c10-dbf1-423d-99cc-7e32453a8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b33a8e6-626f-4644-9fa6-288db4ada647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 1            |        cudaMalloc retries: 1         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  23089 MiB |  23345 MiB |  76549 MiB |  53459 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  23089 MiB |  23345 MiB |  76549 MiB |  53459 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  23041 MiB |  23297 MiB |  76501 MiB |  53459 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  23730 MiB |  23922 MiB |  28198 MiB |   4468 MiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory | 655719 KiB |   1035 MiB |  16622 MiB |  15982 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |    1204    |    1208    |    2567    |    1363    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |    1204    |    1208    |    2567    |    1363    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     469    |     472    |     538    |      69    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |     168    |     168    |     809    |     641    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_summary(device=None, abbreviated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f061fb7b-404a-4d89-b7b7-01a60a3f27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27093857-b53b-44d4-9b6b-63356f8be27e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "96dda477-e44c-4a5c-afc4-dd380b08c4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 4            |        cudaMalloc retries: 4         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  23182 MiB |  23338 MiB | 113648 MiB |  90465 MiB |\n",
      "|       from large pool |  22708 MiB |  22865 MiB | 113163 MiB |  90454 MiB |\n",
      "|       from small pool |    473 MiB |    474 MiB |    484 MiB |     11 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  23182 MiB |  23338 MiB | 113648 MiB |  90465 MiB |\n",
      "|       from large pool |  22708 MiB |  22865 MiB | 113163 MiB |  90454 MiB |\n",
      "|       from small pool |    473 MiB |    474 MiB |    484 MiB |     11 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  23130 MiB |  23286 MiB | 113563 MiB |  90432 MiB |\n",
      "|       from large pool |  22657 MiB |  22813 MiB | 113078 MiB |  90421 MiB |\n",
      "|       from small pool |    473 MiB |    474 MiB |    484 MiB |     10 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  23914 MiB |  23914 MiB |  44396 MiB |  20482 MiB |\n",
      "|       from large pool |  23438 MiB |  23438 MiB |  43920 MiB |  20482 MiB |\n",
      "|       from small pool |    476 MiB |    476 MiB |    476 MiB |      0 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 617811 KiB |   1195 MiB |  23361 MiB |  22758 MiB |\n",
      "|       from large pool | 615431 KiB |   1193 MiB |  23108 MiB |  22507 MiB |\n",
      "|       from small pool |   2380 KiB |      3 MiB |    253 MiB |    250 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1299    |    1302    |    2591    |    1292    |\n",
      "|       from large pool |     474    |     477    |    1167    |     693    |\n",
      "|       from small pool |     825    |     828    |    1424    |     599    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1299    |    1302    |    2591    |    1292    |\n",
      "|       from large pool |     474    |     477    |    1167    |     693    |\n",
      "|       from small pool |     825    |     828    |    1424    |     599    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     503    |     503    |     600    |      97    |\n",
      "|       from large pool |     265    |     265    |     362    |      97    |\n",
      "|       from small pool |     238    |     238    |     238    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |     232    |     234    |     943    |     711    |\n",
      "|       from large pool |     155    |     157    |     376    |     221    |\n",
      "|       from small pool |      77    |      77    |     567    |     490    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9e4ea4c7-1a95-4928-8fb0-a3ef63439fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pynvml as N\n",
    "\n",
    "MB = 1024 * 1024\n",
    "\n",
    "def get_usage(device_index, my_pid):\n",
    "    N.nvmlInit()\n",
    "\n",
    "    handle = N.nvmlDeviceGetHandleByIndex(device_index)\n",
    "\n",
    "    usage = [nv_process.usedGpuMemory // MB for nv_process in\n",
    "             N.nvmlDeviceGetComputeRunningProcesses(handle) + N.nvmlDeviceGetGraphicsRunningProcesses(handle) if\n",
    "             nv_process.pid == my_pid]\n",
    "\n",
    "    if len(usage) == 1:\n",
    "        usage = usage[0]\n",
    "    else:\n",
    "        raise KeyError(\"PID not found\")\n",
    "\n",
    "    return usage\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#    print(get_usage(sys.argv[1], sys.argv[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "22daa2a8-3ed0-4f0c-8da8-556cf6788fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23540"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_usage(0,205798)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3f6ddddc-816a-40cd-9882-9d53f081de0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pkill 205798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a1a511-6921-475d-af8b-357c0610a375",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
